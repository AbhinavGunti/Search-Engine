{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting csv to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('BooksDataSet.csv', 'r',encoding=\"utf-8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    \n",
    "    with open('BookSummaryDataset.txt', 'w',encoding=\"utf-8\") as txt_file:\n",
    "        for row in csv_reader:\n",
    "            txt_file.write('\\t'.join(row) + '\\n')\n",
    "with open('BookSummaryDataset.txt', 'r',encoding=\"utf-8\") as txt_file:\n",
    "        print(txt_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting txt to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Drowned Wednesday</td>\n",
       "      <td>Drowned Wednesday is the first Trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Lost Hero</td>\n",
       "      <td>As the book opens, Jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Eyes of the Overworld</td>\n",
       "      <td>Cugel is easily persuaded by the merchant Fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Magic's Promise</td>\n",
       "      <td>The book opens with Herald-Mage Vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Taran Wanderer</td>\n",
       "      <td>Taran and Gurgi have returned to Caer Dallben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>2995</td>\n",
       "      <td>White Death</td>\n",
       "      <td>A Novel from the NUMA files, A Kurt Austin Ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>2996</td>\n",
       "      <td>Venus with Pistol</td>\n",
       "      <td>Gilbert Kemp is dealer specializing in antiqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2997</td>\n",
       "      <td>Blackwater</td>\n",
       "      <td>\"How do you know when you're in too deep? Dav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2998</td>\n",
       "      <td>The Rainbow and the Rose</td>\n",
       "      <td>The story concerns the life of Johnnie Pascoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2999</td>\n",
       "      <td>Chiefs</td>\n",
       "      <td>The First Chief: Will Henry Lee: The novel op...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Doc_ID                  Book_Name  \\\n",
       "0          0          Drowned Wednesday   \n",
       "1          1              The Lost Hero   \n",
       "2          2  The Eyes of the Overworld   \n",
       "3          3            Magic's Promise   \n",
       "4          4             Taran Wanderer   \n",
       "...      ...                        ...   \n",
       "2995    2995                White Death   \n",
       "2996    2996          Venus with Pistol   \n",
       "2997    2997                 Blackwater   \n",
       "2998    2998   The Rainbow and the Rose   \n",
       "2999    2999                     Chiefs   \n",
       "\n",
       "                                                Summary  \n",
       "0      Drowned Wednesday is the first Trustee among ...  \n",
       "1      As the book opens, Jason awakens on a school ...  \n",
       "2      Cugel is easily persuaded by the merchant Fia...  \n",
       "3      The book opens with Herald-Mage Vanyel return...  \n",
       "4      Taran and Gurgi have returned to Caer Dallben...  \n",
       "...                                                 ...  \n",
       "2995   A Novel from the NUMA files, A Kurt Austin Ad...  \n",
       "2996   Gilbert Kemp is dealer specializing in antiqu...  \n",
       "2997   \"How do you know when you're in too deep? Dav...  \n",
       "2998   The story concerns the life of Johnnie Pascoe...  \n",
       "2999   The First Chief: Will Henry Lee: The novel op...  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"BookSummaryDataset.txt\",delimiter='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhinav\n",
      "[nltk_data]     Gunti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>drowned wednesday is the first trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>as the book opens, jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>cugel is easily persuaded by the merchant fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>the book opens with herald-mage vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>taran and gurgi have returned to caer dallben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>2995</td>\n",
       "      <td>white death</td>\n",
       "      <td>a novel from the numa files, a kurt austin ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>2996</td>\n",
       "      <td>venus with pistol</td>\n",
       "      <td>gilbert kemp is dealer specializing in antiqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2997</td>\n",
       "      <td>blackwater</td>\n",
       "      <td>\"how do you know when you're in too deep? dav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2998</td>\n",
       "      <td>the rainbow and the rose</td>\n",
       "      <td>the story concerns the life of johnnie pascoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2999</td>\n",
       "      <td>chiefs</td>\n",
       "      <td>the first chief: will henry lee: the novel op...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Doc_ID                  Book_Name  \\\n",
       "0          0          drowned wednesday   \n",
       "1          1              the lost hero   \n",
       "2          2  the eyes of the overworld   \n",
       "3          3            magic's promise   \n",
       "4          4             taran wanderer   \n",
       "...      ...                        ...   \n",
       "2995    2995                white death   \n",
       "2996    2996          venus with pistol   \n",
       "2997    2997                 blackwater   \n",
       "2998    2998   the rainbow and the rose   \n",
       "2999    2999                     chiefs   \n",
       "\n",
       "                                                Summary  \n",
       "0      drowned wednesday is the first trustee among ...  \n",
       "1      as the book opens, jason awakens on a school ...  \n",
       "2      cugel is easily persuaded by the merchant fia...  \n",
       "3      the book opens with herald-mage vanyel return...  \n",
       "4      taran and gurgi have returned to caer dallben...  \n",
       "...                                                 ...  \n",
       "2995   a novel from the numa files, a kurt austin ad...  \n",
       "2996   gilbert kemp is dealer specializing in antiqu...  \n",
       "2997   \"how do you know when you're in too deep? dav...  \n",
       "2998   the story concerns the life of johnnie pascoe...  \n",
       "2999   the first chief: will henry lee: the novel op...  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercasing(df):\n",
    "    df_lowercasing=df.copy()\n",
    "    df_lowercasing['Book_Name'] = df['Book_Name'].apply(str.lower)\n",
    "    df_lowercasing['Summary'] = df['Summary'].apply(str.lower)\n",
    "    return df_lowercasing\n",
    "df_lowercasing=lowercasing(df)\n",
    "df_lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"After Loweringcasing : \\n\",df_lowercasing[\"Summary\"][3],\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "# pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def tokenization(df_lowercasing):\n",
    "    df_tokenized=df_lowercasing.copy()\n",
    "    corpus=df_tokenized[\"Summary\"].values\n",
    "    tokenized_summaries = []\n",
    "    for i in range(len(corpus)):\n",
    "        # use loc accessor to modify original DataFrame directly\n",
    "        tokens = WordPunctTokenizer().tokenize(corpus[i])\n",
    "        tokenized_summaries.append(tokens)\n",
    "    df_tokenized[\"Summary\"] = tokenized_summaries\n",
    "    return df_tokenized\n",
    "\n",
    "df_tokenized=tokenization(df_lowercasing)\n",
    "df_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(df_tokenized[\"Summary\"][2997]),\"\\n\",len(df_tokenized[\"Summary\"][2997]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After tokenization : \\n\",\" \".join(df_tokenized[\"Summary\"][3]),\"\\n\",len(df_tokenized[\"Summary\"][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(df_tokenized):\n",
    "    df_punc=df_tokenized.copy()\n",
    "    translator =str.maketrans('', '', string.punctuation+\" \")\n",
    "    for i in range(len(df_tokenized['Summary'])):\n",
    "        df_punc['Summary'][i] = [token.translate(translator) for token in df_tokenized['Summary'][i]]\n",
    "        df_punc['Summary'][i] = list(filter(None, df_punc['Summary'][i])) #to remove space that got generated while removing punctuation\n",
    "\n",
    "    return df_punc\n",
    "df_punc=remove_punctuation(df_tokenized)\n",
    "df_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(df_punc[\"Summary\"][2997]),\"\\n\",len(df_punc[\"Summary\"][2997]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After removing punctuation : \\n\",\" \".join(df_punc[\"Summary\"][3]),\"\\n\",len(df_punc[\"Summary\"][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(\"List of all stopwords in english : \\n\",stopwords.words('english'))\n",
    "print(\"Number of stopwords : \",len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_punc['Summary'])\n",
    "def avg_words_per_document(corpus,length):\n",
    "    avg_words_per_doc=0\n",
    "    for i in corpus:\n",
    "        avg_words_per_doc+=len(i)\n",
    "    avg_words_per_doc=avg_words_per_doc/length\n",
    "    return avg_words_per_doc\n",
    "\n",
    "print(\"Average number of words per document before stopword removal : \",avg_words_per_document(df_punc['Summary'],3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from functools import partial\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(document):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in document if word not in stop_words]\n",
    "\n",
    "def process_documents(documents):\n",
    "    threads = []\n",
    "    for i, document in enumerate(documents):\n",
    "        # create a partial function to call remove_stopwords with both i and document\n",
    "        target_func = partial(remove_stopwords, document) #(func,list)\n",
    "        thread = threading.Thread(target=lambda idx, func: documents.__setitem__(idx, func()), args=(i, target_func))#documents.__setitem__(idx, func()) will modify the list at specified index idx, setting it to thte result of calling function\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        \n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    return documents\n",
    "\n",
    "df_sw=df_punc.copy()\n",
    "corpus=df_sw['Summary']\n",
    "df_sw['Summary'] = process_documents(corpus)\n",
    "print(df_sw['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw['Summary']\n",
    "print(\"Average number of words per document after stopword removal : \",avg_words_per_document(df_tokenized_sw['Summary'],3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After removing punctuation : \\n\",\" \".join(df_punc[\"Summary\"][3]),\"\\n\",len(df_punc[\"Summary\"][3]))\n",
    "\n",
    "print(\"After removing stopwords : \\n\",\" \".join(df_sw['Summary'][3]),\"\\n\",len(df_sw[\"Summary\"][3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatization_df(df_sw):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df_lemmaztized=df_sw.copy()\n",
    "    for i in range(len(df_lemmaztized[\"Summary\"])):\n",
    "        df_lemmaztized['Summary'][i] = [lemmatizer.lemmatize(w) for w in df_sw[\"Summary\"][i]]\n",
    "    return df_lemmaztized\n",
    "df_lemmaztized=lemmatization_df(df_sw)\n",
    "df_lemmaztized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After Lemmatization : \\n\",\" \".join(df_lemmaztized[\"Summary\"][3]),\"\\n\",len(df_lemmaztized[\"Summary\"][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
