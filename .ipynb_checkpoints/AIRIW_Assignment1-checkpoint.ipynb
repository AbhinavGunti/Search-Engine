{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIRIW Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Abhinav Gunti   &emsp; PES1UG20CS008\n",
    "2. Abhishek Singhi &emsp; PES1UG20CS011\n",
    "3. Aniket Acharya  &emsp; PES1UG20CS052\n",
    "4. Anish Khairnar  &emsp; PES1UG20CS058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset : Book Summary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is a text file containing summaries of 3000 books. Each book summary ranges from 500-1000 words in length and is accompanied by a title and an index that starts at 0. The text file consists of a series of lines, with each line representing a single book summary and its associated title and index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting txt to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Drowned Wednesday</td>\n",
       "      <td>Drowned Wednesday is the first Trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Lost Hero</td>\n",
       "      <td>As the book opens, Jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Eyes of the Overworld</td>\n",
       "      <td>Cugel is easily persuaded by the merchant Fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Magic's Promise</td>\n",
       "      <td>The book opens with Herald-Mage Vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Taran Wanderer</td>\n",
       "      <td>Taran and Gurgi have returned to Caer Dallben...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          Drowned Wednesday   \n",
       "1       1              The Lost Hero   \n",
       "2       2  The Eyes of the Overworld   \n",
       "3       3            Magic's Promise   \n",
       "4       4             Taran Wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0   Drowned Wednesday is the first Trustee among ...  \n",
       "1   As the book opens, Jason awakens on a school ...  \n",
       "2   Cugel is easily persuaded by the merchant Fia...  \n",
       "3   The book opens with Herald-Mage Vanyel return...  \n",
       "4   Taran and Gurgi have returned to Caer Dallben...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"BookSummaryDataset.txt\",delimiter='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of words per document :  2717.5523333333335\n"
     ]
    }
   ],
   "source": [
    "avg=0\n",
    "for i in df[\"Summary\"]:\n",
    "    avg+=len(i)\n",
    "avg=avg/(len(df))\n",
    "print(\"Average Number of words per document : \",avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3127"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"Summary\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus after lowercasing : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>drowned wednesday is the first trustee among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>as the book opens, jason awakens on a school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>cugel is easily persuaded by the merchant fia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>the book opens with herald-mage vanyel return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>taran and gurgi have returned to caer dallben...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0   drowned wednesday is the first trustee among ...  \n",
       "1   as the book opens, jason awakens on a school ...  \n",
       "2   cugel is easily persuaded by the merchant fia...  \n",
       "3   the book opens with herald-mage vanyel return...  \n",
       "4   taran and gurgi have returned to caer dallben...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercasing(df):\n",
    "    df_lowercasing=df.copy()\n",
    "    df_lowercasing['Book_Name'] = df['Book_Name'].apply(str.lower)\n",
    "    df_lowercasing['Summary'] = df['Summary'].apply(str.lower)\n",
    "    return df_lowercasing\n",
    "df_lowercasing=lowercasing(df)\n",
    "print(\"Corpus after lowercasing : \\n\")\n",
    "df_lowercasing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>[drowned, wednesday, is, the, first, trustee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>[as, the, book, opens, ,, jason, awakens, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>[cugel, is, easily, persuaded, by, the, mercha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>[the, book, opens, with, herald, -, mage, vany...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>[taran, and, gurgi, have, returned, to, caer, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0  [drowned, wednesday, is, the, first, trustee, ...  \n",
       "1  [as, the, book, opens, ,, jason, awakens, on, ...  \n",
       "2  [cugel, is, easily, persuaded, by, the, mercha...  \n",
       "3  [the, book, opens, with, herald, -, mage, vany...  \n",
       "4  [taran, and, gurgi, have, returned, to, caer, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "def tokenization(df_lowercasing):\n",
    "    df_tokenized=df_lowercasing.copy()\n",
    "    corpus=df_tokenized[\"Summary\"].values\n",
    "    tokenized_summaries = []\n",
    "    for i in range(len(corpus)):\n",
    "        # use loc accessor to modify original DataFrame directly\n",
    "        tokens = WordPunctTokenizer().tokenize(corpus[i])\n",
    "        tokenized_summaries.append(tokens)\n",
    "    df_tokenized[\"Summary\"] = tokenized_summaries\n",
    "    return df_tokenized\n",
    "\n",
    "df_tokenized=tokenization(df_lowercasing)\n",
    "df_tokenized.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>[drowned, wednesday, is, the, first, trustee, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>[as, the, book, opens, jason, awakens, on, a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>[cugel, is, easily, persuaded, by, the, mercha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>[the, book, opens, with, herald, mage, vanyel,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>[taran, and, gurgi, have, returned, to, caer, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0  [drowned, wednesday, is, the, first, trustee, ...  \n",
       "1  [as, the, book, opens, jason, awakens, on, a, ...  \n",
       "2  [cugel, is, easily, persuaded, by, the, mercha...  \n",
       "3  [the, book, opens, with, herald, mage, vanyel,...  \n",
       "4  [taran, and, gurgi, have, returned, to, caer, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "def remove_punctuation(df_tokenized):\n",
    "    df_punc=df_tokenized.copy()\n",
    "    translator =str.maketrans('', '', string.punctuation+\" \")\n",
    "    for i in range(len(df_tokenized['Summary'])):\n",
    "        df_punc['Summary'][i] = [token.translate(translator) for token in df_tokenized['Summary'][i]]\n",
    "        df_punc['Summary'][i] = list(filter(None, df_punc['Summary'][i])) #to remove space that got generated while removing punctuation\n",
    "\n",
    "    return df_punc\n",
    "df_punc=remove_punctuation(df_tokenized)\n",
    "df_punc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: [WinError 10054] An existing\n",
      "[nltk_data]     connection was forcibly closed by the remote host\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all stopwords in english : \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "Number of stopwords :  179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(\"List of all stopwords in english : \\n\",stopwords.words('english'))\n",
    "print(\"Number of stopwords : \",len(stopwords.words('english')))\n",
    "\n",
    "# print(df_punc['Summary'])\n",
    "def avg_words_per_document(corpus,length):\n",
    "    avg_words_per_doc=0\n",
    "    for i in corpus:\n",
    "        avg_words_per_doc+=len(i)\n",
    "    avg_words_per_doc=avg_words_per_doc/length\n",
    "    return avg_words_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per document before stopword removal :  475.325\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of words per document before stopword removal : \",avg_words_per_document(df_punc['Summary'],3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the concept of **multithreading** to remove stopwords to reduce the time taken to remove 179 stopwords from 3000 documents (each document containing around 500-1000 words)\n",
    "\n",
    "We run multiple threads of execution concurrently within a single program. A thread can be used to perform a specific task independently of the main thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [drowned, wednesday, first, trustee, among, mo...\n",
      "1    [book, opens, jason, awakens, school, bus, una...\n",
      "2    [cugel, easily, persuaded, merchant, fianosthe...\n",
      "3    [book, opens, herald, mage, vanyel, returning,...\n",
      "4    [taran, gurgi, returned, caer, dallben, follow...\n",
      "Name: Summary, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from functools import partial\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(document):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in document if word not in stop_words]\n",
    "\n",
    "def process_documents(documents):\n",
    "    threads = []\n",
    "    for i, document in enumerate(documents):\n",
    "        # create a partial function to call remove_stopwords with both i and document\n",
    "        target_func = partial(remove_stopwords, document) #(func,list)\n",
    "        thread = threading.Thread(target=lambda idx, func: documents.__setitem__(idx, func()), args=(i, target_func))#documents.__setitem__(idx, func()) will modify the list at specified index idx, setting it to thte result of calling function\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        \n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    return documents\n",
    "\n",
    "df_sw=df_punc.copy()\n",
    "corpus=df_sw['Summary']\n",
    "df_sw['Summary'] = process_documents(corpus)\n",
    "print(df_sw['Summary'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>[drowned, wednesday, first, trustee, among, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>[book, open, jason, awakens, school, bus, unab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>[cugel, easily, persuaded, merchant, fianosthe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>[book, open, herald, mage, vanyel, returning, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>[taran, gurgi, returned, caer, dallben, follow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0  [drowned, wednesday, first, trustee, among, mo...  \n",
       "1  [book, open, jason, awakens, school, bus, unab...  \n",
       "2  [cugel, easily, persuaded, merchant, fianosthe...  \n",
       "3  [book, open, herald, mage, vanyel, returning, ...  \n",
       "4  [taran, gurgi, returned, caer, dallben, follow...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatization_df(df_sw):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df_lemmaztized=df_sw.copy()\n",
    "    lem=[]\n",
    "    for i in range(len(df_lemmaztized[\"Summary\"])):\n",
    "        lem.append([lemmatizer.lemmatize(w) for w in df_sw[\"Summary\"][i]])\n",
    "    df_lemmaztized['Summary']=lem\n",
    "    return df_lemmaztized\n",
    "df_lemmaztized=lemmatization_df(df_sw)\n",
    "df_lemmaztized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>[drown, wednesday, first, truste, among, morro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>[book, open, jason, awaken, school, bu, unabl,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>[cugel, easili, persuad, merchant, fianosth, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>[book, open, herald, mage, vanyel, return, cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>[taran, gurgi, return, caer, dallben, follow, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0  [drown, wednesday, first, truste, among, morro...  \n",
       "1  [book, open, jason, awaken, school, bu, unabl,...  \n",
       "2  [cugel, easili, persuad, merchant, fianosth, a...  \n",
       "3  [book, open, herald, mage, vanyel, return, cou...  \n",
       "4  [taran, gurgi, return, caer, dallben, follow, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "def stemming(df_sw): \n",
    "    ps = PorterStemmer()\n",
    "    df_stemmed=df_sw.copy()\n",
    "    stem=[]\n",
    "    for i in range(len(df_stemmed[\"Summary\"])):\n",
    "            stem.append([ps.stem(w) for w in df_sw[\"Summary\"][i]])\n",
    "    df_stemmed['Summary']=stem\n",
    "    return df_stemmed\n",
    "df_stemmed=stemming(df_sw)\n",
    "df_stemmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Preprocessed Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Book_Name</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>drowned wednesday</td>\n",
       "      <td>drowned wednesday first trustee among morrow d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the lost hero</td>\n",
       "      <td>book open jason awakens school bus unable reme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the eyes of the overworld</td>\n",
       "      <td>cugel easily persuaded merchant fianosther att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>magic's promise</td>\n",
       "      <td>book open herald mage vanyel returning country...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>taran wanderer</td>\n",
       "      <td>taran gurgi returned caer dallben following ev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_ID                  Book_Name  \\\n",
       "0       0          drowned wednesday   \n",
       "1       1              the lost hero   \n",
       "2       2  the eyes of the overworld   \n",
       "3       3            magic's promise   \n",
       "4       4             taran wanderer   \n",
       "\n",
       "                                             Summary  \n",
       "0  drowned wednesday first trustee among morrow d...  \n",
       "1  book open jason awakens school bus unable reme...  \n",
       "2  cugel easily persuaded merchant fianosther att...  \n",
       "3  book open herald mage vanyel returning country...  \n",
       "4  taran gurgi returned caer dallben following ev...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed=df_lemmaztized.copy()\n",
    "summary=[]\n",
    "for i in range(len(df_lemmaztized[\"Summary\"])):\n",
    "    summary.append(\" \".join(df_lemmaztized['Summary'][i]))\n",
    "df_preprocessed[\"Summary\"]=summary\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We either perform lemmatization or stemming, we dont perform both one after another since it could lead to over normalization of terms !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **dictionary** for inverted index where the key will the term and the value will be a list of document IDs in which the term appears\n",
    "\n",
    "Dictionary : {Key (Term) : Value (Doc Ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index on lemmatized data : \n",
      "Vocabulary Size (Lemmatization) :  45309\n",
      "drowned \t: [0, 172, 306, 312, 374, 376, 379, 424, 431, 659, 975, 1067, 1150, 1180, 1247, 1294, 1384, 1411, 1430, 1435, 1440, 1493, 1530, 1578, 1659, 1840, 1871, 2049, 2054, 2114, 2176, 2296, 2628, 2672, 2929]\n",
      "wednesday \t: [0, 1844, 2284]\n",
      "first \t: [0, 1, 4, 10, 14, 18, 19, 33, 34, 38, 41, 47, 50, 53, 54, 57, 60, 62, 66, 67, 69, 71, 73, 75, 76, 79, 80, 82, 83, 87, 88, 89, 101, 103, 106, 109, 112, 115, 118, 121, 124, 128, 130, 131, 133, 134, 139, 140, 147, 149, 151, 156, 157, 158, 159, 160, 161, 162, 163, 167, 168, 169, 170, 172, 175, 178, 181, 184, 185, 186, 188, 191, 192, 193, 194, 195, 196, 202, 205, 209, 214, 228, 229, 230, 232, 233, 236, 239, 240, 245, 248, 249, 250, 252, 257, 261, 269, 272, 276, 278, 279, 280, 284, 290, 293, 294, 296, 299, 306, 310, 311, 312, 313, 315, 316, 317, 318, 321, 323, 324, 327, 329, 334, 337, 341, 343, 344, 345, 349, 359, 361, 363, 365, 367, 368, 370, 372, 375, 377, 380, 385, 389, 391, 394, 395, 397, 398, 399, 408, 410, 414, 416, 417, 418, 419, 422, 423, 425, 427, 429, 430, 432, 433, 435, 436, 442, 444, 446, 447, 449, 451, 453, 454, 457, 458, 460, 471, 473, 475, 477, 478, 479, 481, 483, 486, 487, 488, 489, 490, 491, 497, 498, 499, 501, 502, 503, 505, 506, 508, 510, 511, 512, 513, 516, 521, 524, 528, 530, 531, 534, 536, 537, 538, 544, 545, 546, 555, 556, 557, 558, 559, 564, 569, 570, 572, 575, 576, 578, 579, 585, 587, 590, 594, 597, 601, 603, 604, 605, 606, 608, 613, 615, 618, 626, 627, 630, 631, 632, 634, 636, 637, 644, 646, 648, 651, 652, 658, 660, 664, 668, 670, 673, 677, 678, 681, 684, 687, 688, 689, 690, 691, 694, 697, 708, 709, 711, 714, 716, 718, 726, 727, 728, 730, 732, 736, 738, 739, 743, 745, 746, 750, 752, 754, 756, 764, 769, 770, 771, 773, 776, 777, 780, 781, 785, 789, 793, 804, 806, 812, 815, 819, 820, 822, 824, 825, 827, 829, 833, 842, 843, 844, 847, 848, 850, 852, 861, 873, 878, 884, 885, 888, 891, 893, 895, 896, 911, 914, 916, 923, 925, 928, 931, 932, 937, 939, 940, 945, 949, 951, 954, 955, 957, 959, 960, 961, 965, 969, 974, 975, 979, 982, 984, 986, 988, 989, 990, 992, 994, 995, 997, 1001, 1002, 1010, 1015, 1018, 1019, 1020, 1024, 1025, 1026, 1030, 1034, 1037, 1051, 1053, 1056, 1064, 1066, 1071, 1074, 1078, 1081, 1085, 1088, 1090, 1092, 1097, 1101, 1102, 1103, 1109, 1111, 1115, 1120, 1121, 1126, 1135, 1138, 1140, 1142, 1149, 1150, 1152, 1158, 1159, 1162, 1164, 1167, 1172, 1173, 1174, 1175, 1176, 1177, 1180, 1185, 1187, 1194, 1197, 1200, 1202, 1206, 1207, 1208, 1209, 1211, 1213, 1215, 1219, 1220, 1221, 1222, 1230, 1232, 1243, 1247, 1251, 1254, 1261, 1263, 1265, 1268, 1269, 1272, 1275, 1276, 1277, 1278, 1282, 1288, 1290, 1306, 1309, 1314, 1315, 1323, 1328, 1330, 1339, 1340, 1345, 1346, 1351, 1352, 1356, 1358, 1365, 1368, 1370, 1374, 1380, 1383, 1384, 1386, 1394, 1403, 1409, 1412, 1413, 1415, 1418, 1422, 1434, 1435, 1436, 1440, 1441, 1445, 1446, 1455, 1458, 1460, 1461, 1462, 1463, 1469, 1473, 1477, 1479, 1482, 1489, 1492, 1499, 1502, 1504, 1513, 1516, 1522, 1524, 1528, 1530, 1535, 1536, 1539, 1541, 1550, 1552, 1556, 1559, 1560, 1561, 1565, 1566, 1569, 1573, 1575, 1576, 1577, 1578, 1581, 1585, 1586, 1596, 1597, 1601, 1606, 1607, 1608, 1610, 1614, 1618, 1620, 1634, 1638, 1642, 1644, 1645, 1647, 1648, 1653, 1656, 1658, 1661, 1662, 1663, 1664, 1667, 1669, 1670, 1672, 1676, 1682, 1687, 1688, 1689, 1692, 1693, 1694, 1696, 1699, 1700, 1702, 1703, 1705, 1707, 1708, 1711, 1712, 1715, 1723, 1726, 1727, 1728, 1730, 1732, 1734, 1741, 1745, 1746, 1747, 1750, 1752, 1759, 1760, 1769, 1771, 1774, 1777, 1784, 1787, 1793, 1795, 1796, 1798, 1802, 1804, 1805, 1807, 1811, 1812, 1813, 1815, 1816, 1817, 1818, 1820, 1821, 1822, 1824, 1829, 1832, 1835, 1838, 1840, 1842, 1845, 1848, 1853, 1857, 1860, 1862, 1864, 1869, 1870, 1871, 1874, 1878, 1879, 1881, 1882, 1885, 1887, 1890, 1892, 1893, 1894, 1898, 1899, 1902, 1904, 1905, 1908, 1912, 1913, 1917, 1919, 1920, 1921, 1922, 1925, 1926, 1927, 1928, 1937, 1940, 1942, 1945, 1949, 1950, 1952, 1953, 1956, 1957, 1958, 1960, 1966, 1967, 1968, 1971, 1972, 1976, 1977, 1978, 1980, 1982, 1983, 1984, 1986, 1990, 1993, 1995, 1997, 1999, 2000, 2001, 2002, 2003, 2004, 2006, 2011, 2013, 2023, 2026, 2030, 2031, 2033, 2037, 2038, 2052, 2054, 2058, 2060, 2070, 2071, 2072, 2074, 2075, 2076, 2081, 2085, 2086, 2088, 2089, 2092, 2093, 2094, 2097, 2099, 2104, 2110, 2114, 2117, 2118, 2125, 2129, 2133, 2134, 2144, 2147, 2148, 2149, 2150, 2152, 2155, 2159, 2162, 2167, 2168, 2169, 2170, 2178, 2179, 2180, 2181, 2182, 2186, 2192, 2201, 2206, 2207, 2212, 2214, 2216, 2219, 2220, 2221, 2224, 2228, 2233, 2234, 2242, 2247, 2250, 2251, 2252, 2258, 2259, 2261, 2265, 2271, 2272, 2276, 2280, 2281, 2282, 2293, 2296, 2298, 2303, 2305, 2313, 2318, 2321, 2323, 2333, 2336, 2337, 2338, 2339, 2342, 2346, 2349, 2350, 2351, 2352, 2357, 2359, 2364, 2370, 2371, 2374, 2376, 2382, 2383, 2386, 2388, 2391, 2392, 2396, 2398, 2399, 2405, 2408, 2422, 2424, 2432, 2436, 2439, 2440, 2443, 2447, 2448, 2449, 2459, 2461, 2462, 2470, 2471, 2476, 2477, 2498, 2500, 2502, 2505, 2511, 2517, 2519, 2522, 2525, 2531, 2537, 2541, 2548, 2551, 2552, 2559, 2561, 2564, 2567, 2570, 2572, 2575, 2577, 2578, 2579, 2580, 2582, 2587, 2588, 2590, 2592, 2601, 2606, 2609, 2612, 2615, 2617, 2625, 2627, 2629, 2635, 2636, 2639, 2642, 2644, 2647, 2650, 2656, 2657, 2659, 2661, 2663, 2674, 2677, 2679, 2686, 2689, 2691, 2692, 2694, 2697, 2699, 2710, 2714, 2716, 2717, 2718, 2720, 2723, 2733, 2734, 2737, 2739, 2740, 2741, 2742, 2744, 2747, 2749, 2751, 2752, 2753, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2764, 2767, 2772, 2773, 2775, 2778, 2784, 2785, 2792, 2793, 2794, 2795, 2796, 2797, 2810, 2814, 2817, 2826, 2827, 2829, 2830, 2843, 2845, 2849, 2850, 2854, 2855, 2860, 2863, 2864, 2873, 2875, 2880, 2881, 2884, 2885, 2886, 2893, 2897, 2899, 2902, 2908, 2910, 2911, 2921, 2923, 2925, 2929, 2930, 2934, 2938, 2940, 2943, 2945, 2950, 2952, 2955, 2957, 2963, 2964, 2966, 2968, 2969, 2975, 2977, 2979, 2982, 2986, 2989, 2999]\n",
      "trustee \t: [0, 444, 537, 556, 1033, 1215, 1306, 1862, 1874, 2785]\n",
      "among \t: [0, 4, 6, 7, 16, 30, 46, 50, 60, 67, 70, 71, 75, 87, 106, 133, 138, 141, 148, 157, 158, 163, 171, 180, 183, 188, 194, 222, 250, 278, 279, 310, 311, 318, 333, 335, 349, 361, 381, 387, 395, 406, 424, 425, 447, 448, 451, 473, 486, 491, 498, 503, 505, 521, 538, 547, 577, 579, 590, 593, 608, 610, 617, 636, 658, 659, 662, 666, 672, 674, 696, 707, 712, 720, 725, 730, 734, 777, 782, 798, 812, 821, 845, 847, 851, 852, 861, 873, 879, 882, 883, 887, 889, 893, 896, 899, 908, 917, 933, 934, 950, 951, 954, 955, 962, 966, 985, 988, 990, 999, 1045, 1090, 1092, 1102, 1112, 1115, 1123, 1140, 1185, 1205, 1226, 1263, 1269, 1306, 1356, 1396, 1440, 1454, 1499, 1502, 1504, 1520, 1522, 1525, 1539, 1549, 1591, 1600, 1613, 1632, 1634, 1640, 1651, 1660, 1667, 1669, 1671, 1672, 1673, 1685, 1687, 1705, 1712, 1715, 1724, 1727, 1737, 1754, 1760, 1761, 1784, 1793, 1794, 1805, 1809, 1824, 1863, 1866, 1870, 1876, 1887, 1894, 1897, 1913, 1924, 1926, 1939, 1945, 1947, 1953, 1958, 1962, 1966, 1972, 1979, 1983, 1994, 1997, 2016, 2055, 2065, 2074, 2086, 2103, 2111, 2120, 2129, 2160, 2168, 2179, 2188, 2225, 2233, 2235, 2242, 2276, 2284, 2292, 2296, 2313, 2337, 2339, 2347, 2350, 2361, 2364, 2392, 2405, 2423, 2434, 2443, 2445, 2446, 2448, 2449, 2451, 2461, 2475, 2492, 2519, 2553, 2569, 2583, 2587, 2588, 2594, 2602, 2647, 2688, 2692, 2694, 2703, 2720, 2759, 2761, 2788, 2839, 2862, 2863, 2865, 2887, 2899, 2902, 2906, 2913, 2940, 2952, 2954, 2987]\n",
      "Inverted index on stemmed data : \n",
      "Vocabulary Size (Stemming) :  35766\n",
      "drowned \t: [0, 172, 306, 312, 374, 376, 379, 424, 431, 659, 975, 1067, 1150, 1180, 1247, 1294, 1384, 1411, 1430, 1435, 1440, 1493, 1530, 1578, 1659, 1840, 1871, 2049, 2054, 2114, 2176, 2296, 2628, 2672, 2929]\n",
      "wednesday \t: [0, 1844, 2284]\n",
      "first \t: [0, 1, 4, 10, 14, 18, 19, 33, 34, 38, 41, 47, 50, 53, 54, 57, 60, 62, 66, 67, 69, 71, 73, 75, 76, 79, 80, 82, 83, 87, 88, 89, 101, 103, 106, 109, 112, 115, 118, 121, 124, 128, 130, 131, 133, 134, 139, 140, 147, 149, 151, 156, 157, 158, 159, 160, 161, 162, 163, 167, 168, 169, 170, 172, 175, 178, 181, 184, 185, 186, 188, 191, 192, 193, 194, 195, 196, 202, 205, 209, 214, 228, 229, 230, 232, 233, 236, 239, 240, 245, 248, 249, 250, 252, 257, 261, 269, 272, 276, 278, 279, 280, 284, 290, 293, 294, 296, 299, 306, 310, 311, 312, 313, 315, 316, 317, 318, 321, 323, 324, 327, 329, 334, 337, 341, 343, 344, 345, 349, 359, 361, 363, 365, 367, 368, 370, 372, 375, 377, 380, 385, 389, 391, 394, 395, 397, 398, 399, 408, 410, 414, 416, 417, 418, 419, 422, 423, 425, 427, 429, 430, 432, 433, 435, 436, 442, 444, 446, 447, 449, 451, 453, 454, 457, 458, 460, 471, 473, 475, 477, 478, 479, 481, 483, 486, 487, 488, 489, 490, 491, 497, 498, 499, 501, 502, 503, 505, 506, 508, 510, 511, 512, 513, 516, 521, 524, 528, 530, 531, 534, 536, 537, 538, 544, 545, 546, 555, 556, 557, 558, 559, 564, 569, 570, 572, 575, 576, 578, 579, 585, 587, 590, 594, 597, 601, 603, 604, 605, 606, 608, 613, 615, 618, 626, 627, 630, 631, 632, 634, 636, 637, 644, 646, 648, 651, 652, 658, 660, 664, 668, 670, 673, 677, 678, 681, 684, 687, 688, 689, 690, 691, 694, 697, 708, 709, 711, 714, 716, 718, 726, 727, 728, 730, 732, 736, 738, 739, 743, 745, 746, 750, 752, 754, 756, 764, 769, 770, 771, 773, 776, 777, 780, 781, 785, 789, 793, 804, 806, 812, 815, 819, 820, 822, 824, 825, 827, 829, 833, 842, 843, 844, 847, 848, 850, 852, 861, 873, 878, 884, 885, 888, 891, 893, 895, 896, 911, 914, 916, 923, 925, 928, 931, 932, 937, 939, 940, 945, 949, 951, 954, 955, 957, 959, 960, 961, 965, 969, 974, 975, 979, 982, 984, 986, 988, 989, 990, 992, 994, 995, 997, 1001, 1002, 1010, 1015, 1018, 1019, 1020, 1024, 1025, 1026, 1030, 1034, 1037, 1051, 1053, 1056, 1064, 1066, 1071, 1074, 1078, 1081, 1085, 1088, 1090, 1092, 1097, 1101, 1102, 1103, 1109, 1111, 1115, 1120, 1121, 1126, 1135, 1138, 1140, 1142, 1149, 1150, 1152, 1158, 1159, 1162, 1164, 1167, 1172, 1173, 1174, 1175, 1176, 1177, 1180, 1185, 1187, 1194, 1197, 1200, 1202, 1206, 1207, 1208, 1209, 1211, 1213, 1215, 1219, 1220, 1221, 1222, 1230, 1232, 1243, 1247, 1251, 1254, 1261, 1263, 1265, 1268, 1269, 1272, 1275, 1276, 1277, 1278, 1282, 1288, 1290, 1306, 1309, 1314, 1315, 1323, 1328, 1330, 1339, 1340, 1345, 1346, 1351, 1352, 1356, 1358, 1365, 1368, 1370, 1374, 1380, 1383, 1384, 1386, 1394, 1403, 1409, 1412, 1413, 1415, 1418, 1422, 1434, 1435, 1436, 1440, 1441, 1445, 1446, 1455, 1458, 1460, 1461, 1462, 1463, 1469, 1473, 1477, 1479, 1482, 1489, 1492, 1499, 1502, 1504, 1513, 1516, 1522, 1524, 1528, 1530, 1535, 1536, 1539, 1541, 1550, 1552, 1556, 1559, 1560, 1561, 1565, 1566, 1569, 1573, 1575, 1576, 1577, 1578, 1581, 1585, 1586, 1596, 1597, 1601, 1606, 1607, 1608, 1610, 1614, 1618, 1620, 1634, 1638, 1642, 1644, 1645, 1647, 1648, 1653, 1656, 1658, 1661, 1662, 1663, 1664, 1667, 1669, 1670, 1672, 1676, 1682, 1687, 1688, 1689, 1692, 1693, 1694, 1696, 1699, 1700, 1702, 1703, 1705, 1707, 1708, 1711, 1712, 1715, 1723, 1726, 1727, 1728, 1730, 1732, 1734, 1741, 1745, 1746, 1747, 1750, 1752, 1759, 1760, 1769, 1771, 1774, 1777, 1784, 1787, 1793, 1795, 1796, 1798, 1802, 1804, 1805, 1807, 1811, 1812, 1813, 1815, 1816, 1817, 1818, 1820, 1821, 1822, 1824, 1829, 1832, 1835, 1838, 1840, 1842, 1845, 1848, 1853, 1857, 1860, 1862, 1864, 1869, 1870, 1871, 1874, 1878, 1879, 1881, 1882, 1885, 1887, 1890, 1892, 1893, 1894, 1898, 1899, 1902, 1904, 1905, 1908, 1912, 1913, 1917, 1919, 1920, 1921, 1922, 1925, 1926, 1927, 1928, 1937, 1940, 1942, 1945, 1949, 1950, 1952, 1953, 1956, 1957, 1958, 1960, 1966, 1967, 1968, 1971, 1972, 1976, 1977, 1978, 1980, 1982, 1983, 1984, 1986, 1990, 1993, 1995, 1997, 1999, 2000, 2001, 2002, 2003, 2004, 2006, 2011, 2013, 2023, 2026, 2030, 2031, 2033, 2037, 2038, 2052, 2054, 2058, 2060, 2070, 2071, 2072, 2074, 2075, 2076, 2081, 2085, 2086, 2088, 2089, 2092, 2093, 2094, 2097, 2099, 2104, 2110, 2114, 2117, 2118, 2125, 2129, 2133, 2134, 2144, 2147, 2148, 2149, 2150, 2152, 2155, 2159, 2162, 2167, 2168, 2169, 2170, 2178, 2179, 2180, 2181, 2182, 2186, 2192, 2201, 2206, 2207, 2212, 2214, 2216, 2219, 2220, 2221, 2224, 2228, 2233, 2234, 2242, 2247, 2250, 2251, 2252, 2258, 2259, 2261, 2265, 2271, 2272, 2276, 2280, 2281, 2282, 2293, 2296, 2298, 2303, 2305, 2313, 2318, 2321, 2323, 2333, 2336, 2337, 2338, 2339, 2342, 2346, 2349, 2350, 2351, 2352, 2357, 2359, 2364, 2370, 2371, 2374, 2376, 2382, 2383, 2386, 2388, 2391, 2392, 2396, 2398, 2399, 2405, 2408, 2422, 2424, 2432, 2436, 2439, 2440, 2443, 2447, 2448, 2449, 2459, 2461, 2462, 2470, 2471, 2476, 2477, 2498, 2500, 2502, 2505, 2511, 2517, 2519, 2522, 2525, 2531, 2537, 2541, 2548, 2551, 2552, 2559, 2561, 2564, 2567, 2570, 2572, 2575, 2577, 2578, 2579, 2580, 2582, 2587, 2588, 2590, 2592, 2601, 2606, 2609, 2612, 2615, 2617, 2625, 2627, 2629, 2635, 2636, 2639, 2642, 2644, 2647, 2650, 2656, 2657, 2659, 2661, 2663, 2674, 2677, 2679, 2686, 2689, 2691, 2692, 2694, 2697, 2699, 2710, 2714, 2716, 2717, 2718, 2720, 2723, 2733, 2734, 2737, 2739, 2740, 2741, 2742, 2744, 2747, 2749, 2751, 2752, 2753, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2764, 2767, 2772, 2773, 2775, 2778, 2784, 2785, 2792, 2793, 2794, 2795, 2796, 2797, 2810, 2814, 2817, 2826, 2827, 2829, 2830, 2843, 2845, 2849, 2850, 2854, 2855, 2860, 2863, 2864, 2873, 2875, 2880, 2881, 2884, 2885, 2886, 2893, 2897, 2899, 2902, 2908, 2910, 2911, 2921, 2923, 2925, 2929, 2930, 2934, 2938, 2940, 2943, 2945, 2950, 2952, 2955, 2957, 2963, 2964, 2966, 2968, 2969, 2975, 2977, 2979, 2982, 2986, 2989, 2999]\n",
      "trustee \t: [0, 444, 537, 556, 1033, 1215, 1306, 1862, 1874, 2785]\n",
      "among \t: [0, 4, 6, 7, 16, 30, 46, 50, 60, 67, 70, 71, 75, 87, 106, 133, 138, 141, 148, 157, 158, 163, 171, 180, 183, 188, 194, 222, 250, 278, 279, 310, 311, 318, 333, 335, 349, 361, 381, 387, 395, 406, 424, 425, 447, 448, 451, 473, 486, 491, 498, 503, 505, 521, 538, 547, 577, 579, 590, 593, 608, 610, 617, 636, 658, 659, 662, 666, 672, 674, 696, 707, 712, 720, 725, 730, 734, 777, 782, 798, 812, 821, 845, 847, 851, 852, 861, 873, 879, 882, 883, 887, 889, 893, 896, 899, 908, 917, 933, 934, 950, 951, 954, 955, 962, 966, 985, 988, 990, 999, 1045, 1090, 1092, 1102, 1112, 1115, 1123, 1140, 1185, 1205, 1226, 1263, 1269, 1306, 1356, 1396, 1440, 1454, 1499, 1502, 1504, 1520, 1522, 1525, 1539, 1549, 1591, 1600, 1613, 1632, 1634, 1640, 1651, 1660, 1667, 1669, 1671, 1672, 1673, 1685, 1687, 1705, 1712, 1715, 1724, 1727, 1737, 1754, 1760, 1761, 1784, 1793, 1794, 1805, 1809, 1824, 1863, 1866, 1870, 1876, 1887, 1894, 1897, 1913, 1924, 1926, 1939, 1945, 1947, 1953, 1958, 1962, 1966, 1972, 1979, 1983, 1994, 1997, 2016, 2055, 2065, 2074, 2086, 2103, 2111, 2120, 2129, 2160, 2168, 2179, 2188, 2225, 2233, 2235, 2242, 2276, 2284, 2292, 2296, 2313, 2337, 2339, 2347, 2350, 2361, 2364, 2392, 2405, 2423, 2434, 2443, 2445, 2446, 2448, 2449, 2451, 2461, 2475, 2492, 2519, 2553, 2569, 2583, 2587, 2588, 2594, 2602, 2647, 2688, 2692, 2694, 2703, 2720, 2759, 2761, 2788, 2839, 2862, 2863, 2865, 2887, 2899, 2902, 2906, 2913, 2940, 2952, 2954, 2987]\n"
     ]
    }
   ],
   "source": [
    "def inverted_index(df):\n",
    "    inverted_index_dict={}\n",
    "    for doc_id in range(len(df)):\n",
    "        for term in df['Summary'][doc_id]:\n",
    "            if term not in inverted_index_dict:\n",
    "                inverted_index_dict[term]=[]\n",
    "            if doc_id not in inverted_index_dict[term]:\n",
    "                inverted_index_dict[term].append(doc_id)\n",
    "\n",
    "    return inverted_index_dict\n",
    "\n",
    "inverted_index_lemmatized=inverted_index(df_lemmaztized)\n",
    "inverted_index_stemmed=inverted_index(df_stemmed)\n",
    "# print(\"Inverted index on lemmatized data : \",inverted_index_lemmatized)\n",
    "print(\"Inverted index on lemmatized data : \")\n",
    "print(\"Vocabulary Size (Lemmatization) : \", len(inverted_index_lemmatized))\n",
    "for key in list(inverted_index_lemmatized.keys())[:5]:\n",
    "    print(f\"{key} \\t: {inverted_index_lemmatized[key]}\")\n",
    "# print(\"Inverted index on stemmed data : \",inverted_index_stemmed)\n",
    "print(\"Inverted index on stemmed data : \")\n",
    "print(\"Vocabulary Size (Stemming) : \", len(inverted_index_stemmed))\n",
    "for key in list(inverted_index_lemmatized.keys())[:5]:\n",
    "    print(f\"{key} \\t: {inverted_index_lemmatized[key]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Query Processing With An Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Query processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import string\n",
    " \n",
    "def preprocess_query(query):#lemmatization, stemming , lowercasing, punctuation\n",
    "    #lowercasing\n",
    "    query=query.lower()\n",
    "    # print(\"Lowercased : \",query)\n",
    "\n",
    "    #tokenization\n",
    "    query=wordpunct_tokenize(query)\n",
    "    # print(\"Tokenized query : \",query)\n",
    "    #removing punctuation\n",
    "    translator =str.maketrans('', '', string.punctuation+\" \")\n",
    "    query=[token.translate(translator) for token in query]\n",
    "    query=list(filter(None, query))\n",
    "    # print(\"Removed punctuation : \",query)\n",
    "\n",
    "    #stopword removal except and,or,not\n",
    "    stop_words = set(stopwords.words('english'))-{'and','or','not'}\n",
    "    query = [word for word in query if word not in stop_words]\n",
    "    # print(\"Removed stopwords except logical operators : \",query)\n",
    "\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_query_terms=[lemmatizer.lemmatize(w) for w in query if w not in ['AND','OR','NOT']]\n",
    "    # print(\"Lemmatized query : \",lemmatized_query_terms)\n",
    "\n",
    "    #stemming\n",
    "    ps=PorterStemmer()\n",
    "    stemmed_query_terms=[ps.stem(w) for w in query if w not in ['and','or','not']]\n",
    "    # print(\"Stemmed query : \",stemmed_query_terms)\n",
    "\n",
    "    return lemmatized_query_terms,stemmed_query_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting processed query (infix) to postfix expression and evaluating postfix expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infix_to_postfix(expr):\n",
    "    prec = {\"not\": 3, \"and\": 2, \"or\": 1}        #operator precedence\n",
    "    output = []         #output stack\n",
    "    op_stack = []       #operator stack\n",
    "    tokens = expr\n",
    "    for token in tokens:\n",
    "        if token in prec:\n",
    "            # If the token is an operator, pop operators from the operator stack and append them to the output\n",
    "            # until the stack is empty or the top operator has lower precedence than the current operator.\n",
    "            while op_stack and op_stack[-1] in prec and prec[op_stack[-1]] >= prec[token]:\n",
    "                output.append(op_stack.pop())\n",
    "            op_stack.append(token)\n",
    "        else:\n",
    "            # If the token is an operand, append it to the output.\n",
    "            output.append(token)\n",
    "    # Pop any remaining operators from the operator stack and append them to the output\n",
    "    while op_stack:\n",
    "        output.append(op_stack.pop())\n",
    "    # Join the output list into a string and return it\n",
    "    return output\n",
    "\n",
    "def eval_postfix(lst):\n",
    "    stack = []\n",
    "    for token in lst:\n",
    "        if isinstance(token, list):\n",
    "            # If the token is an operand, push it onto the stack\n",
    "            stack.append(token)\n",
    "        elif token == \"not\":\n",
    "            # If the token is \"not\", pop the top operand from the stack, negate it, and push the result back onto the stack\n",
    "            operand = stack.pop()\n",
    "            # result = [x for x in range(len(df))]-operand\n",
    "            corpus=[x for x in range(3000)]\n",
    "            result=[x for x in corpus if x not in operand]\n",
    "            result.sort()\n",
    "            print(result)\n",
    "            stack.append(result)\n",
    "        elif token == \"and\":\n",
    "            # If the token is \"and\", pop the top two operands from the stack, perform a logical \"and\" operation, and push the result back onto the stack\n",
    "            operand2 = stack.pop()\n",
    "            operand1 = stack.pop()\n",
    "            # print(\"Operands : \",operand1,operand2)\n",
    "            result = [value for value in operand1 if value in operand2]\n",
    "            result.sort()\n",
    "            stack.append(result)\n",
    "        elif token == \"or\":\n",
    "            # If the token is \"or\", pop the top two operands from the stack, perform a logical \"or\" operation, and push the result back onto the stack\n",
    "            operand2 = stack.pop()\n",
    "            operand1 = stack.pop()\n",
    "            result = list(set(operand1) | set(operand2))\n",
    "            result.sort()\n",
    "            stack.append(result)\n",
    "        else:\n",
    "            # If the token is an unknown operator or operand, raise an error\n",
    "            raise ValueError(f\"Unknown token {token}\")\n",
    "    # The final result is the only element left on the stack\n",
    "    return stack.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query_processing_using_inverted_index(query):\n",
    "    #preprocessing the query\n",
    "    print(\"QUERY : \",query)\n",
    "    query_lemmatized,query_stemmed=preprocess_query(query)\n",
    "\n",
    "    #converting infix query to postfix query\n",
    "    postfix_expr=infix_to_postfix(query_lemmatized)\n",
    "\n",
    "    # print(postfix_expr)\n",
    "\n",
    "    #mapping terms to their inverted indices\n",
    "    inverted_list_terms=[]\n",
    "    for i in postfix_expr:\n",
    "        if i not in ['and','or','not']:\n",
    "            if i not in inverted_index_lemmatized:\n",
    "                return f\"Word \\\"{i}\\\" not found in corpus\"\n",
    "            inverted_list_terms.append(inverted_index_lemmatized[i])\n",
    "        else:\n",
    "            inverted_list_terms.append(i)\n",
    "    # print(\"Inverted list terms : \",inverted_list_terms)\n",
    "\n",
    "    #Evaluating the postfix expression to fetch the required documents\n",
    "    retreived_documents=eval_postfix(inverted_list_terms)\n",
    "    # retreived_documents.sort()\n",
    "    print(\"Retreived Documents : \",retreived_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY :  journey and mountain and rocks\n",
      "Retreived Documents :  [244, 410, 590, 2150, 2244, 2405]\n"
     ]
    }
   ],
   "source": [
    "boolean_query_processing_using_inverted_index(\"journey and mountain and rocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY :  monitoring and not moon\n",
      "[0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 52, 53, 54, 55, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314, 315, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 371, 372, 373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 604, 605, 606, 607, 608, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 627, 628, 629, 630, 631, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 676, 677, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 712, 713, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 738, 739, 740, 741, 742, 743, 744, 745, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 791, 792, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 807, 809, 810, 811, 812, 813, 814, 816, 817, 818, 819, 820, 822, 823, 825, 826, 828, 829, 830, 831, 832, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 854, 855, 856, 857, 859, 860, 861, 862, 863, 864, 866, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 881, 882, 883, 884, 885, 886, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 980, 981, 982, 983, 984, 985, 986, 987, 988, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1210, 1211, 1212, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2071, 2072, 2073, 2074, 2076, 2077, 2078, 2079, 2080, 2082, 2083, 2084, 2085, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2149, 2150, 2151, 2152, 2153, 2154, 2156, 2157, 2158, 2159, 2160, 2161, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2197, 2198, 2199, 2200, 2202, 2203, 2204, 2205, 2207, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999]\n",
      "Retreived Documents :  [217, 611, 672, 1019, 2626, 2661, 2809, 2940]\n"
     ]
    }
   ],
   "source": [
    "boolean_query_processing_using_inverted_index(\"monitoring and not moon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY :  Tuscany or carpenter\n",
      "Retreived Documents :  [29, 54, 345, 745, 1037, 1251, 1263, 1282, 1360, 1583, 1630, 1681, 1727, 1871, 1890, 1908, 2196, 2301, 2840, 2873, 2934, 2969]\n"
     ]
    }
   ],
   "source": [
    "boolean_query_processing_using_inverted_index(\"Tuscany or carpenter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Query \n",
    "* Biword Index\n",
    "* Extended Biwords index (NXN Biwords)\n",
    "\n",
    "Biword indexing and extended biword indexing are methods of implementation of phrase query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biword index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most frequent Biwords :  [('year old', 410), ('next day', 273), ('new york', 272), ('take place', 230), ('united state', 182), ('one day', 171), ('jean claude', 155), ('fall love', 152), ('year later', 150), ('year ago', 139)]\n"
     ]
    }
   ],
   "source": [
    "def biword_index(df):\n",
    "    biword_dict = {}\n",
    "    biword_count={}\n",
    "\n",
    "    for doc in range(len(df)):\n",
    "        words = df['Summary'][doc]\n",
    "        # print(\"Length : \",len(words))\n",
    "        # Create bi-words by pairing adjacent words\n",
    "        \n",
    "        biwords = [words[i] + \" \" + words[i+1] for i in range(len(words)-1)]\n",
    "        # Add each bi-word to the index\n",
    "        # print(\"Biword length : \",len(biwords))\n",
    "        for biword in biwords:\n",
    "            if biword in biword_dict:\n",
    "                biword_count[biword]+=1\n",
    "                if doc not in biword_dict[biword]:\n",
    "                    biword_dict[biword].append(doc)\n",
    "            else:\n",
    "                biword_dict[biword] = [doc]\n",
    "                biword_count[biword]=1\n",
    "\n",
    "\n",
    "    # Print the bi-word index\n",
    "    # print(biword_dict)\n",
    "    # print(biword_count)\n",
    "\n",
    "    # print(len(biword_dict))\n",
    "    # print(len(biword_count))\n",
    "    # print(len(biword_dict[biwords[0]]))\n",
    "    sorted_biword_count = dict(sorted(biword_count.items(), key=lambda item: item[1], reverse=True))\n",
    "    print(\"Top 10 Most frequent Biwords : \",list(sorted_biword_count.items())[:10])\n",
    "\n",
    "    return biword_dict,biword_count,sorted_biword_count\n",
    "\n",
    "biword_dict,biword_count,sorted_biword_count=biword_index(df_lemmaztized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617573"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biword_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Biwords (NX*N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: [WinError 10054]\n",
      "[nltk_data]     An existing connection was forcibly closed by the\n",
      "[nltk_data]     remote host\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import concurrent.futures\n",
    "\n",
    "def generate_extended_biwords(df):\n",
    "    # Convert text to lowercase and remove all non-alphabetic characters\n",
    "    \n",
    "    # Tokenize text into a list of words\n",
    "    # words = nltk.word_tokenize(text)\n",
    "    extended_biwords = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for doc in range(len(df)):\n",
    "            futures.append(executor.submit(process_document, df['Summary'][doc]))\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            extended_biwords.extend(future.result())\n",
    "    return extended_biwords\n",
    "\n",
    "def process_document(doc):\n",
    "    # Perform POS tagging on the words\n",
    "    tagged_words = nltk.pos_tag(doc)\n",
    "    # Initialize a list to store the extended biwords\n",
    "    extended_biwords = []\n",
    "    # Loop through each tagged word in the list\n",
    "    for i, tagged_word in enumerate(tagged_words):\n",
    "        word, tag = tagged_word\n",
    "        # If the word is a noun, add it to the extended biword\n",
    "        if tag.startswith('N'):\n",
    "            extended_biword = word\n",
    "            # Look ahead for the next noun or the end of the list\n",
    "            for j in range(i+1, len(tagged_words)):\n",
    "                next_word, next_tag = tagged_words[j]\n",
    "                # If the next word is a noun, add it to the extended biword\n",
    "                if next_tag.startswith('N'):\n",
    "                    extended_biword += \" \" + next_word\n",
    "                    extended_biwords.append(extended_biword)\n",
    "                    break\n",
    "                # If the next word is an article or preposition, add it to the extended biword\n",
    "                elif next_tag.startswith('D') or next_tag.startswith('I'):\n",
    "                    extended_biword += \" \" + next_word\n",
    "                # If the next word is not a noun, article, or preposition, stop looking ahead\n",
    "                else:\n",
    "                    break\n",
    "    return extended_biwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_biwords = generate_extended_biwords(df_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Few of the extended biwords : \",extended_biwords[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considering NXN biwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN=[i for i in extended_biwords if len(i.split())==2]\n",
    "NXN=[i for i in extended_biwords if len(i.split())==3]\n",
    "NXXN=[i for i in extended_biwords if len(i.split())==4]\n",
    "NXXXN=[i for i in extended_biwords if len(i.split())==5]\n",
    "\n",
    "print(\"Total number of extended biwords : \",len(extended_biwords))\n",
    "print(\"\\nNumber of NN : \",len(NN))\n",
    "print(\"Number of NXN : \",len(NXN))\n",
    "print(\"Number of NXXN : \",len(NXXN))\n",
    "print(\"Number of NXXXN : \",len(NXXXN))\n",
    "\n",
    "print(\"\\nFew NN extended biwords : \",NN[:5])\n",
    "print(\"Few NXN extended biwords : \",NXN[:5])\n",
    "print(\"Few NXXN extended biwords : \",NXXN[:5])\n",
    "print(\"Few NXXXN extended biwords : \",NXXXN[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented using intesect algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_positional_index(documents):\n",
    "    \"\"\"\n",
    "    This method builds a positional index for a list of documents.\n",
    "    \"\"\"\n",
    "    index = {}\n",
    "    for doc_id, document in enumerate(documents):\n",
    "        for position, token in enumerate(document):\n",
    "            if token not in index:\n",
    "                index[token] = {}\n",
    "            if doc_id not in index[token]:\n",
    "                index[token][doc_id] = []\n",
    "            index[token][doc_id].append(position)\n",
    "    return index\n",
    "def preprocess_query_positional_index(query):#lemmatization, stemming , lowercasing, punctuation\n",
    "    #lowercasing\n",
    "    query=query.lower()\n",
    "    # print(\"Lowercased : \",query)\n",
    "\n",
    "    #tokenization\n",
    "    query=wordpunct_tokenize(query)\n",
    "    # print(\"Tokenized query : \",query)\n",
    "    #removing punctuation\n",
    "    translator =str.maketrans('', '', string.punctuation+\" \")\n",
    "    query=[token.translate(translator) for token in query]\n",
    "    query=list(filter(None, query))\n",
    "    # print(\"Removed punctuation : \",query)\n",
    "\n",
    "    #stopword removal except and,or,not\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    query = [word for word in query if word not in stop_words]\n",
    "    # print(\"Removed stopwords except logical operators : \",query)\n",
    "\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_query_terms=[lemmatizer.lemmatize(w) for w in query if w not in ['AND','OR','NOT']]\n",
    "    # print(\"Lemmatized query : \",lemmatized_query_terms)\n",
    "\n",
    "    #stemming\n",
    "    ps=PorterStemmer()\n",
    "    stemmed_query_terms=[ps.stem(w) for w in query if w not in ['and','or','not']]\n",
    "    # print(\"Stemmed query : \",stemmed_query_terms)\n",
    "\n",
    "    return lemmatized_query_terms,stemmed_query_terms\n",
    "def search(index, query):\n",
    "    \"\"\"\n",
    "    This method retrieves documents that match a given query in the positional index.\n",
    "    \"\"\"\n",
    "    # Split the query into tokens\n",
    "    query_tokens = query\n",
    "\n",
    "    # Initialize a set to store the candidate documents\n",
    "    candidates = list(range(len(index[list(index.keys())[0]])))\n",
    "\n",
    "    # Iterate through each query token and intersect the candidate set with the set of documents\n",
    "    # that contain the token at the next position\n",
    "    for i, token in enumerate(query_tokens):\n",
    "        if token not in index:\n",
    "            return \"No documents retreived (Token not in corpus)\"\n",
    "        if i == 0:\n",
    "            candidates = list(index[token].keys())\n",
    "            # print(i,\" First Candidate : \",token)\n",
    "            # print(\"Doc IDs of first candidate : \\n\",candidates)\n",
    "        else:\n",
    "            new_candidates = list(index[token].keys())\n",
    "            candidates = list(set(candidates).intersection(set(new_candidates)))\n",
    "            # print(i,\" New Candidate\",token)\n",
    "            # print(\"New Canditate Doc IDs\",new_candidates)\n",
    "            # print(\"Intersect of curr and prev\",candidates)\n",
    "            # print(\"Intersect Candidates : \",candidates)\n",
    "\n",
    "            for doc_id in candidates:\n",
    "                positions = index[token][doc_id]    #1. [1]\n",
    "                # print(\"Token : \",token,\"\\n Positions : \",positions)\n",
    "                s=0\n",
    "                for j in range(len(positions)):\n",
    "                    # print(positions[j],i,(positions[j] - 1) not in index[query_tokens[i-1]][doc_id])\n",
    "                    # print(candidates)\n",
    "                    \n",
    "                    if (positions[j] - 1) in index[query_tokens[i-1]][doc_id]:\n",
    "                        s=1\n",
    "                        # print(s)\n",
    "                # print(doc_id)\n",
    "                if s==0:      #(1-1) not in index[born][33]\n",
    "                    # print((positions[j] - i),candidates)\n",
    "                    candidates.remove(doc_id)\n",
    "                    # print(\"Outside : \",(positions[j] - 1),candidates)\n",
    "\n",
    "                    break\n",
    "\n",
    "    # Sort the candidate set by the number of matches, and return the documents in descending order\n",
    "    # of the number of matches\n",
    "    candidate_scores = [(doc_id, sum([len(index[token][doc_id]) for token in query_tokens])) for doc_id in candidates]\n",
    "    candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [doc_id for (doc_id, score) in candidate_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_index=build_positional_index(df_lemmaztized[\"Summary\"])\n",
    "print(\"Positional Index of first 10 terms in the vocabulary : \\n\")\n",
    "for i in list(positional_index.keys())[:10]:\n",
    "    print(f\"{i}\\n {positional_index[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"chased police\"\n",
    "# query=input()\n",
    "query_lemmatized,query_stemmed=preprocess_query_positional_index(query)\n",
    "retreived_documents_positional_index=search(positional_index,query_lemmatized)\n",
    "print(\"QUERY : \",query)\n",
    "print(\"Query after preprocessing : \",query_lemmatized)\n",
    "print(\"Retreived Documents (Higer precedence to lower): \",retreived_documents_positional_index)\n",
    "print(\"Number of documents retreived : \",len(retreived_documents_positional_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Proximity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented by modifying Intersect algorithm For Proximity Constraint K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query_proximity_search(query):\n",
    "    query_terms=query.split(' ')\n",
    "    term1=query_terms[0]\n",
    "    term2=query_terms[2]\n",
    "    k=query_terms[1][1:]\n",
    "    return term1,term2,k\n",
    "\n",
    "preprocess_query_proximity_search(\"rise /4 halfway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_search(query,positional_index):\n",
    "    term1,term2,k=preprocess_query_proximity_search(query)\n",
    "    k=int(k)\n",
    "    print(term1,term2,k)\n",
    "    if (term1 not in positional_index) or (term2 not in positional_index):\n",
    "        return \"No documents retreived (Token not found in corpus)\"\n",
    "    pos1_index=positional_index[term1]\n",
    "    pos2_index=positional_index[term2]\n",
    "    ret_docs=[]\n",
    "    doc_ids=[x for x in pos1_index if x in pos2_index]\n",
    "    for i in doc_ids:\n",
    "        positions=positional_index[term1][i]\n",
    "        for j in positional_index[term2][i]:\n",
    "            for x in positions:\n",
    "                if ((j-x<=k) and i not in ret_docs):      #words appearing with k places of each other\n",
    "                    ret_docs.append(i)\n",
    "    return ret_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retreived Documents: \",proximity_search(\"extra /100 work\",positional_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wildcard Queries\n",
    "&nbsp; 1. WILDCARD QUERIES on Inverted Index  \n",
    "&nbsp; 2. WILDCARD QUERIES on Positional Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WILDCARD QUERIES on Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wildcard queries on inverted index\n",
    "import re\n",
    "def wildcard_entry_inverted_index(query, inverted_index_dict):\n",
    "    retreived_docs=set()\n",
    "    matching_terms=[]\n",
    "    if query.endswith('*'):                     #leading wildcard entries\n",
    "        query_terms = query.split('*')\n",
    "        # print(query_terms)\n",
    "        query_terms=query_terms[0]\n",
    "        for i in inverted_index_dict:\n",
    "            if (i.startswith(query_terms)):\n",
    "                retreived_docs.update(inverted_index_dict[i])\n",
    "                matching_terms.append(i)\n",
    "\n",
    "    elif query.startswith(\"*\"):                 #trailing wildcard entries\n",
    "        query_terms = query.split('*')\n",
    "        # print(query_terms)\n",
    "        query_terms=query_terms[1]\n",
    "        for i in inverted_index_dict:\n",
    "            if (i.endswith(query_terms)):\n",
    "                retreived_docs.update(inverted_index_dict[i])\n",
    "                matching_terms.append(i)\n",
    "    else :                                      #widcard in between\n",
    "        if \"*\" in query:\n",
    "            l=len(query)-1\n",
    "        else:\n",
    "            l=len(query)                    \n",
    "        query_terms = query.split('*')\n",
    "        # print(query_terms)\n",
    "        for i in inverted_index_lemmatized:\n",
    "            if (i.startswith(query_terms[0]) and i.endswith(query_terms[1]) and len(i)>=l):\n",
    "                retreived_docs.update(inverted_index_dict[i])\n",
    "                matching_terms.append(i)\n",
    "    return retreived_docs,matching_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leading wild card query (Ex. mon*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_term=\"row*\"\n",
    "matching_docs,matching_terms = wildcard_entry_inverted_index(query_term, inverted_index_lemmatized)\n",
    "print(\"Matching documents : \", matching_docs)\n",
    "print(\"matching Terms : \",matching_terms)\n",
    "print(\"Number of matched documents : \",len(matching_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trailing wild card query (Ex. *mon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_term = \"*hello\"\n",
    "matching_docs,matching_terms = wildcard_entry_inverted_index(query_term, inverted_index_lemmatized)\n",
    "print(\"Matching documents:\", matching_docs)\n",
    "print(\"matching Terms : \",matching_terms)\n",
    "print(len(matching_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Wildcard in-between letters Ex. hel*ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_term = \"pr*ent\"\n",
    "matching_docs,matching_terms= wildcard_entry_inverted_index(query_term, inverted_index_lemmatized)\n",
    "print(\"Matching documents:\", matching_docs)\n",
    "print(\"matching Terms : \",matching_terms)\n",
    "\n",
    "print(len(matching_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WILDCARD QUERIES on Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wildcard_entry_positionl_index(query,positional_index):\n",
    "    retreived_documents=set()\n",
    "    matching_terms=[]\n",
    "    if query.endswith(\"*\"):\n",
    "        query_terms=query.split(\"*\")\n",
    "        query_terms=query_terms[0]\n",
    "        for i in positional_index.keys():\n",
    "            if (i.startswith(query_terms)):\n",
    "                retreived_documents.update(positional_index[i])\n",
    "                matching_terms.append(i)\n",
    "    elif query.startswith(\"*\"):\n",
    "        query_terms=query.split(\"*\")\n",
    "        query_terms=query_terms[1]\n",
    "        for i in positional_index.keys():\n",
    "            if (i.endswith(query_terms)):\n",
    "                retreived_documents.update(positional_index[i])\n",
    "                matching_terms.append(i)\n",
    "    else:\n",
    "        if \"*\" in query:\n",
    "            l=len(query)-1\n",
    "        else:\n",
    "            l=len(query)\n",
    "        query_terms=query.split(\"*\")\n",
    "        for i in positional_index.keys():\n",
    "            if (i.startswith(query_terms[0]) and i.endswith(query_terms[1]) and len(i)):\n",
    "                retreived_documents.update(positional_index[i])\n",
    "                matching_terms.append(i)\n",
    "    wildcard_positional_index_match={}\n",
    "    for i in matching_terms:\n",
    "        wildcard_positional_index_match[i]=positional_index[i]\n",
    "    return retreived_documents,matching_terms,wildcard_positional_index_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leading wild card query (Ex. mon*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"row*\"\n",
    "retreived_documents,matching_terms,wildcard_positional_index_match=wildcard_entry_positionl_index(query,positional_index)\n",
    "print(\"Number of documents retreived : \",len(retreived_documents))\n",
    "print(\"Matching documents:\", retreived_documents)\n",
    "print(\"matching Terms : \",matching_terms)\n",
    "print(\"Positional index of matching terms : \",wildcard_positional_index_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trailing wild card query (Ex. *mon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"*hello\"\n",
    "retreived_documents,matching_terms,wildcard_positional_index_match=wildcard_entry_positionl_index(query,positional_index)\n",
    "print(\"Number of documents retreived : \",len(retreived_documents))\n",
    "print(\"Matching documents:\", retreived_documents)\n",
    "print(\"matching Terms : \",matching_terms)\n",
    "print(\"Positional index of matching terms : \",wildcard_positional_index_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Wildcard in-between letters Ex. hel*ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"pro*ent\"\n",
    "retreived_documents,matching_terms,wildcard_positional_index_match=wildcard_entry_positionl_index(query,positional_index)\n",
    "print(\"Number of documents retreived : \",len(retreived_documents))\n",
    "print(\"Matching documents:\", retreived_documents)\n",
    "print(\"matching Terms : \",matching_terms)\n",
    "print(\"Positional index of matching terms : \",wildcard_positional_index_match)\n",
    "print(len(retreived_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Wild Card Query (Ex se\\*ate AND fil\\*er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infix_to_postfix_wild_card(expr):\n",
    "    prec = {\"not\": 3, \"and\": 2, \"or\": 1}        #operator precedence\n",
    "    output = []         #output stack\n",
    "    op_stack = []       #operator stack\n",
    "    tokens = expr\n",
    "    for token in tokens:\n",
    "        if type(token)!=type([]) and token in prec:\n",
    "            while op_stack and op_stack[-1] in prec and prec[op_stack[-1]] >= prec[token]:\n",
    "                output.append(op_stack.pop())\n",
    "            op_stack.append(token)\n",
    "        else:\n",
    "            output.append(token)\n",
    "    while op_stack:\n",
    "        output.append(op_stack.pop())\n",
    "    return output\n",
    "\n",
    "def eval_postfix(lst):\n",
    "    stack = []\n",
    "    for token in lst:\n",
    "        if isinstance(token, list):\n",
    "            # If the token is an operand, push it onto the stack\n",
    "            stack.append(token)\n",
    "        elif token == \"not\":\n",
    "            # If the token is \"not\", pop the top operand from the stack, negate it, and push the result back onto the stack\n",
    "            operand = stack.pop()\n",
    "            # result = [x for x in range(len(df))]-operand\n",
    "            corpus=[x for x in range(3000)]\n",
    "            result=[x for x in corpus if x not in operand]\n",
    "            result.sort()\n",
    "            print(result)\n",
    "            stack.append(result)\n",
    "        elif token == \"and\":\n",
    "            # If the token is \"and\", pop the top two operands from the stack, perform a logical \"and\" operation, and push the result back onto the stack\n",
    "            operand2 = stack.pop()\n",
    "            operand1 = stack.pop()\n",
    "            # print(\"Operands : \",operand1,operand2)\n",
    "            result = [value for value in operand1 if value in operand2]\n",
    "            result.sort()\n",
    "            stack.append(result)\n",
    "        elif token == \"or\":\n",
    "            # If the token is \"or\", pop the top two operands from the stack, perform a logical \"or\" operation, and push the result back onto the stack\n",
    "            operand2 = stack.pop()\n",
    "            operand1 = stack.pop()\n",
    "            result = list(set(operand1) | set(operand2))\n",
    "            result.sort()\n",
    "            stack.append(result)\n",
    "        else:\n",
    "            # If the token is an unknown operator or operand, raise an error\n",
    "            raise ValueError(f\"Unknown token {token}\")\n",
    "    # The final result is the only element left on the stack\n",
    "    return stack.pop()\n",
    "\n",
    "def compound_wild_card_query(query):\n",
    "    query=query.lower()\n",
    "    query_terms=query.split()\n",
    "    terms=[]\n",
    "    docs=list()\n",
    "    for i in query_terms:\n",
    "        if \"*\" in i:\n",
    "            w,m=wildcard_entry_inverted_index(i,inverted_index_lemmatized)\n",
    "            terms.append(m)\n",
    "            d=set()\n",
    "            for i in m:\n",
    "                d.update(inverted_index_lemmatized[i])\n",
    "            d=list(d)\n",
    "            d.sort()\n",
    "            docs.append(d)\n",
    "        else:\n",
    "            terms.append(i)\n",
    "            docs.append(i)\n",
    "    postfix_expr=infix_to_postfix_wild_card(docs)\n",
    "    print(\"Postfix Expression : \",postfix_expr)\n",
    "    ret=eval_postfix(postfix_expr)\n",
    "    return ret,terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retreived_documents,matched_terms=compound_wild_card_query(\"se*ate and fil*er or pro*ent\")\n",
    "print(\"Retreived Documents : \",retreived_documents)\n",
    "print(\"Total Documents Retreived : \",len(retreived_documents))\n",
    "print(\"Matched Terms : \",matched_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve relevant text using similarity index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using TF-IDF vectorizer to embedd documents and query**  \n",
    "**Using Cosine Similarity to find documents closest to the query and returning them in sorted order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_cosine_similaity(df_preprocessed,query):\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "        corpus=df_preprocessed[\"Summary\"].tolist()\n",
    "        corpus_query=corpus+[query]\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(corpus_query)\n",
    "        features=tfidf_vectorizer.get_feature_names_out()\n",
    "        # print(len(features))\n",
    "        # print(features)\n",
    "        # print(tfidf_matrix.shape)\n",
    "\n",
    "        cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "        return cosine_similarities,tfidf_matrix\n",
    "\n",
    "query=\"point view mysterious stranger also known x renegade ethical contacting people valley enlisting aid posed engineer barry thorne airship\"\n",
    "# query='book begin point view mysterious stranger also known x renegade ethical contacting people valley enlisting aid posed engineer barry thorne airship parseval murdered milton firebrass several others fellow ethicals posing mayan named ah qaaq traveling company chinese poet li po internal reverie reveals secret identity discovered monat grrautut director riverworld project monat recalled dark tower judged however used remote command kill inhabitant tower stopped resurrection agent valley could return tower stuck valley like others began make way upriver hoping catch ride one riverboats reverie end one traumatic day riverworld day left bank grailstones fail fire done meteorite sam clemens discovered broke circuit powered time damage repaired ethicals time either dead stuck valley one day grailstones fail inhabitant left bank invade right en masse half humanity dy conflict time richard burton friend joined crew king john ship rex grandissimus participate defense boat grailstones fail burton masquerading dark age british warrior eventually becomes king john security chief one day tom mix jack london peter jairus frigate apply join crew burton recognizing frigate attack realizing mistake burton eventually becomes friendly frigate becomes ally mix london false frigate disappeared travelling companion monat grrautut boarded sam clemens ship shortly boarding monat murdered renegade ethical eventually two riverboats reach virolando wide lake last inhabited stretch headwater also home pacifist church second chance hermann göring become priest king john clemens finally get square initially begin aerial dog fight plane destroyed two great riverboats attack ensuing conflict riverboats sunk crew die two captain killed king john killed clemens clemens dy heart attack pulled water mortal enemy erik bloodaxe clemens dy notice bloodaxe become adherent church second chance seek revenge clemens among survivor burton frigate alice kaz joe miller li po ah qaaq nur ed din joined american piano player tom turpin english novelist aphra behn man claiming gilgamesh french soldier jean marcelin baron de marbot take one ship launch craft survive fight proceed upriver scale waterfall end river make way much difficulty polar sea arrive tower unmask ah qaaq renegade ethical take prisoner explains identity loga son king priam troy like child died age five resurrected century ago raised alien ethicals one agent reveals also purpose riverworld meant grand moral test allow humanity progress point achieve enlightenment project brought close soul achieved enlightenment set loose wander universe aimlessly loga became obsessed sparing earthly family fate worked sabotage project one awakened burton prematurely recruited clemens others used satellite direct meteorite land near clemens revealed discover problem computer run tower part malfunctioned since one around repair problem reached critical point nothing done computer die releasing stored wathans soul people died riverworld without wathans people never brought back life addition monat locked user accessing computer göring followed group tower attempt fix computer killed security measure put place monat alice devise way get around programming deactivate security loga able repair computer'\n",
    "cosine_similarities,tfidf_matrix=tfidf_cosine_similaity(df_preprocessed,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sorted_cosine_similarities(cosine_similarities):\n",
    "    df_cosine_similarities={}\n",
    "    for i in range(len(df_preprocessed)):\n",
    "        df_cosine_similarities[i]=cosine_similarities[0][i]\n",
    "    sorted_cosine_similarities = dict(sorted(df_cosine_similarities.items(), key=lambda item: item[1],reverse= True))\n",
    "    return sorted_cosine_similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cosine_similarities=df_sorted_cosine_similarities(cosine_similarities)\n",
    "print(\"Top 10 documents retreived : \")\n",
    "print(\"Doc ID\\t Similarity Score\")\n",
    "for i in list(sorted_cosine_similarities.keys())[:10]:\n",
    "    print(f\"{i}\\t: {sorted_cosine_similarities[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve relevant text using liklelihood language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### liklelihood language model -1 : Using Probabilistic (inverse-document-frequency) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import string\n",
    " \n",
    "def likleihood_model_query_preprocessing(query):#lemmatization, stemming , lowercasing, punctuation\n",
    "    #lowercasing\n",
    "    query=query.lower()\n",
    "    # print(\"Lowercased : \",query)\n",
    "\n",
    "    #tokenization\n",
    "    query=wordpunct_tokenize(query)\n",
    "    # print(\"Tokenized query : \",query)\n",
    "    #removing punctuation\n",
    "    translator =str.maketrans('', '', string.punctuation+\" \")\n",
    "    query=[token.translate(translator) for token in query]\n",
    "    query=list(filter(None, query))\n",
    "    # print(\"Removed punctuation : \",query)\n",
    "\n",
    "    #stopword removal except and,or,not\n",
    "    stop_words = set(stopwords.words('english'))-{'and','or','not'}\n",
    "    query = [word for word in query if word not in stop_words]\n",
    "    # print(\"Removed stopwords except logical operators : \",query)\n",
    "\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_query_terms=[lemmatizer.lemmatize(w) for w in query if w not in ['AND','OR','NOT']]\n",
    "    # print(\"Lemmatized query : \",lemmatized_query_terms)\n",
    "\n",
    "    #stemming\n",
    "    ps=PorterStemmer()\n",
    "    stemmed_query_terms=[ps.stem(w) for w in query if w not in ['and','or','not']]\n",
    "    # print(\"Stemmed query : \",stemmed_query_terms)\n",
    "\n",
    "    return lemmatized_query_terms,stemmed_query_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Example input query\n",
    "query = \"irish man\"\n",
    "\n",
    "# preprocessing query\n",
    "query_lemmatized_terms,query_stemmed_terms = likleihood_model_query_preprocessing(query)\n",
    "# print(\"Processed Query : \",query_lemmatized_terms)\n",
    "\n",
    "\n",
    "# Calculate document frequencies\n",
    "doc_freq = {}\n",
    "for doc in df_lemmaztized[\"Summary\"]:\n",
    "    for word in set(doc):\n",
    "        if word in doc_freq:\n",
    "            doc_freq[word] += 1\n",
    "        else:\n",
    "            doc_freq[word] = 1\n",
    "\n",
    "# Calculate inverse document frequencies\n",
    "num_docs = len(df_lemmaztized)\n",
    "inv_doc_freq = {}\n",
    "for word in doc_freq:\n",
    "    inv_doc_freq[word] = math.log(num_docs / doc_freq[word])\n",
    "\n",
    "# Calculate likelihood scores for each document\n",
    "doc_scores = []\n",
    "for doc in df_lemmaztized[\"Summary\"]:\n",
    "    score = 0\n",
    "    for word in query_lemmatized_terms:\n",
    "        if word in doc:\n",
    "            score += inv_doc_freq[word]\n",
    "    doc_scores.append(score)\n",
    "\n",
    "# Sort documents by score and print top results\n",
    "results = sorted(zip(df_lemmaztized[\"Doc_ID\"], doc_scores), key=lambda x: x[1], reverse=True)\n",
    "print(\"Document ID's of top 10 documents : \\n\")\n",
    "for i in range(min(len(results), 10)):\n",
    "    print(f\"Document {results[i][0]}, Score --> {results[i][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Likelihood Model-2 (A Machine Language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist, LidstoneProbDist\n",
    "from nltk.lm import Vocabulary, MLE, Laplace, Lidstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_docs(corpus):\n",
    "#     processed_docs = []\n",
    "#     for doc in corpus:\n",
    "#         # tokenization\n",
    "#         tokens = word_tokenize(doc)\n",
    "#         # remove numbers and single-character tokens\n",
    "#         processed_doc = [token.lower() for token in tokens if not token.isnumeric() and len(token) > 1]\n",
    "#         processed_docs.append(processed_doc)\n",
    "#     return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = [word for doc in df_lemmaztized[\"Summary\"] for word in doc]\n",
    "# len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_likelihood_language_model(processed_docs, smoothing='mle', gamma=0.1):\n",
    "    # create vocabulary\n",
    "    words = [word for doc in processed_docs for word in doc]\n",
    "    vocab = Vocabulary(words, unk_cutoff=1)\n",
    "    \n",
    "    # create frequency distribution\n",
    "    freq_dist = FreqDist(words)\n",
    "    \n",
    "    # create probability distribution\n",
    "    if smoothing == 'mle':\n",
    "        prob_dist = MLE(freq_dist)\n",
    "    elif smoothing == 'laplace':\n",
    "        prob_dist = Laplace(freq_dist)\n",
    "    elif smoothing == 'lidstone':\n",
    "        prob_dist = LidstoneProbDist(freq_dist, gamma, bins=len(vocab))\n",
    "    else:\n",
    "        raise ValueError('Invalid smoothing method.')\n",
    "    \n",
    "    # create language model\n",
    "    lm = nltk.lm.models.Lidstone(order=2, gamma=gamma, vocabulary=vocab)\n",
    "    lm.fit([list(nltk.ngrams(doc, 2)) for doc in processed_docs])\n",
    "    \n",
    "\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc_id, doc in enumerate(df_lemmaztized[\"Summary\"]):\n",
    "#     print(doc_id,doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"primarily\" in lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_docs(query, processed_docs, lm, num_docs=10):\n",
    "    # preprocess query\n",
    "    # query = query.lower().split()\n",
    "    query_lemmatized,query_stemmed=likleihood_model_query_preprocessing(query)\n",
    "\n",
    "    # retrieve relevant docs\n",
    "    doc_scores = {}\n",
    "    for doc_id, doc in enumerate(processed_docs):\n",
    "        score = 0\n",
    "        for word in query_lemmatized:\n",
    "            if word in lm.vocab:\n",
    "                score += lm.score(word, doc)\n",
    "        doc_scores[doc_id] = score\n",
    "    # print(doc_scores)\n",
    "    # sort docs by relevance score\n",
    "    relevant_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # get content of relevant docs\n",
    "    content = []\n",
    "    for doc_id, score in relevant_docs:\n",
    "        doc = df_lemmaztized[\"Summary\"][doc_id]\n",
    "        content.append((doc_id, score, doc))\n",
    "    \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = train_likelihood_language_model(df_lemmaztized[\"Summary\"], smoothing='mle', gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'take place primarily victorian london story begin mysterious brown leather man enters george watch shop strange device need repair claiming'\n",
    "relevant_documents = retrieve_relevant_docs(query, df_lemmaztized[\"Summary\"], lm, num_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, score, doc in relevant_documents[:5]:\n",
    "    print(f'Relevance score: {score:.64f}\\n{doc_id} -- {doc}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced search: relevance feedback, semantic matching, reranking of results, finding out query intention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Relevance feedback : Rocchio’ 1971 algorithm(SMART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def rocchio_algorithm(query_vector, relevant_docs, non_relevant_docs, alpha=1, beta=0.75, gamma=0.15):\n",
    "    \"\"\"\n",
    "    query_vector: list of floats representing the initial query vector\n",
    "    relevant_docs: list of lists where each inner list represents a relevant document vector\n",
    "    non_relevant_docs: list of lists where each inner list represents a non-relevant document vector\n",
    "    alpha: float representing the weight of the initial query vector\n",
    "    beta: float representing the weight of the relevant documents\n",
    "    gamma: float representing the weight of the non-relevant documents\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the centroid of the relevant and non-relevant documents\n",
    "    relevant_centroid = np.mean(relevant_docs, axis=0)\n",
    "    irrelevant_centroid = np.mean(non_relevant_docs, axis=0)\n",
    "    # Update the query vector using Rocchio's formula\n",
    "    updated_query_vector = alpha * query_vector + beta * relevant_centroid - gamma * irrelevant_centroid\n",
    "    return updated_query_vector\n",
    "# This function takes as input an initial `query_vector`, a list of `relevant_docs` and `non_relevant_docs` (each represented as a list of floats), and optional parameters `alpha`, `beta`, and `gamma` representing the weights for the initial query vector, relevant documents, and non-relevant documents respectively.\n",
    "# The function calculates the centroid of both the relevant and non-relevant documents and then updates the query vector using Rocchio's formula. The updated query vector is returned.\n",
    "# You can use this function to update your query vector based on relevance feedback from users over a collection of 3000 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_query=\"point view mysterious stranger also known x renegade ethical contacting people valley enlisting aid posed engineer barry thorne airship\"\n",
    "cosine_similarities,tfidf_matrix=tfidf_cosine_similaity(df_preprocessed,initial_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix=tfidf_matrix.toarray()\n",
    "initial_query_tfidf=tfidf_matrix[-1]\n",
    "sorted_cosine_similarities_relevance=df_sorted_cosine_similarities(cosine_similarities)\n",
    "relevant_docs=[x for x in list(sorted_cosine_similarities_relevance.values())[:10]]\n",
    "non_relevant_docs=[x for x in list(sorted_cosine_similarities_relevance.values())[10:]]\n",
    "\n",
    "print(len(relevant_docs),len(non_relevant_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_query_vector=rocchio_algorithm(initial_query_tfidf,relevant_docs, non_relevant_docs, alpha=1, beta=0.75, gamma=0.15)\n",
    "next_query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit Relevance Feedback : Rocchio’ 1971 algorithm(SMART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the query vector using the Rocchio algorithm\n",
    "def explicit_rocchio_algorithm(initial_query_tfidf,tfidf_matrix,relevant_docs,irrelevant_docs,alpha=1, beta=0.75, gamma=0.15):\n",
    "    relevant_vectors = tfidf_matrix[list(relevant_docs)]\n",
    "    irrelevant_vectors = tfidf_matrix[list(irrelevant_docs)]\n",
    "    if len(relevant_docs)==0:\n",
    "        relevant_mean=0\n",
    "    else:\n",
    "        relevant_mean = np.mean(relevant_vectors, axis=0)\n",
    "    if len(irrelevant_docs)==0:\n",
    "        irrelevant_mean=0\n",
    "    else:\n",
    "        irrelevant_mean = np.mean(irrelevant_vectors, axis=0)\n",
    "\n",
    "    updated_query_vector = alpha * initial_query_tfidf + beta * relevant_mean - gamma * irrelevant_mean\n",
    "\n",
    "    # Get new top-10 results\n",
    "    scores = cosine_similarity(np.reshape(updated_query_vector, (1, -1)), tfidf_matrix[:-1])[0]\n",
    "    updated_top_10 = np.argsort(scores)[::-1][:10]\n",
    " \n",
    "    \n",
    "\n",
    "    return updated_query_vector,updated_top_10,scores\n",
    "     \n",
    "# explicit_rocchio_algorithm(initial_query_tfidf,tfidf_matrix,relevant_docs,irrelevant_docs,alpha=1, beta=0.75, gamma=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_query=\"point view mysterious stranger also known x renegade ethical contacting people valley enlisting aid posed engineer barry thorne airship\"\n",
    "cosine_similarities,tfidf_matrix=tfidf_cosine_similaity(df_preprocessed,initial_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix=tfidf_matrix.toarray()\n",
    "initial_query_tfidf=tfidf_matrix[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cosine_similarities_relevance=df_sorted_cosine_similarities(cosine_similarities)\n",
    "top_10_relevant_docs={}\n",
    "for doc_id in list(sorted_cosine_similarities_relevance.keys())[:10]:\n",
    "    top_10_relevant_docs[doc_id]=sorted_cosine_similarities_relevance[doc_id]\n",
    "print(\"Doc ID \\tSimilarity Score\\tBook Title\\t\\t\\tSummary\")\n",
    "for i, doc_id in enumerate(top_10_relevant_docs):\n",
    "    book_title=df[\"Book_Name\"][doc_id]\n",
    "    book_summary=df[\"Summary\"][doc_id]\n",
    "    print(f\"{doc_id}\\t{top_10_relevant_docs[doc_id]}\\t{book_title}\\t{book_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    update=input(\"Do you want to give feedback ? (yes/no)\")\n",
    "    update=update.lower()\n",
    "    relevant_docs = set()\n",
    "    irrelevant_docs = set()\n",
    "    if update==\"yes\":\n",
    "        while True:\n",
    "            feedback = input(\"Enter 'r' for relevant, 'i' for irrelevant, or 'q' to quit: \")\n",
    "            if feedback == \"r\":\n",
    "                doc_id = int(input(\"Enter the relevant document number: \"))\n",
    "                relevant_docs.add(doc_id)\n",
    "            elif feedback == \"i\":\n",
    "                doc_id = int(input(\"Enter the irrelevant document number: \"))\n",
    "                irrelevant_docs.add(doc_id)\n",
    "            elif feedback == \"q\":\n",
    "                break\n",
    "        \n",
    "        print(\"\\nDocuments found relevant by the user : \",list(relevant_docs),\"\\nDocuments found irrelevant by the user : \",list(irrelevant_docs))\n",
    "        updated_query_vector,updated_top_10,scores=explicit_rocchio_algorithm(initial_query_tfidf,tfidf_matrix,relevant_docs,irrelevant_docs,alpha=1, beta=0.75, gamma=0.15)\n",
    "        print(\"Doc ID \\tSimilarity Score\\tBook Title\\t\\t\\tSummary\")\n",
    "        for i, doc_id in enumerate(updated_top_10):\n",
    "            book_title=df[\"Book_Name\"][doc_id]\n",
    "            book_summary=df[\"Summary\"][doc_id]\n",
    "            print(f\"{doc_id}\\t{scores[doc_id]}\\t{book_title}\\t{book_summary}\")\n",
    "        initial_query_tfidf=updated_query_vector\n",
    "    else:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
